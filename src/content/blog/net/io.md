---
title: 'I/O模型'
author: '安'
description: '瞅瞅I/O模型做的那些事儿'
publishDate: '2025-08-28'
updatedDate: '2025-08-28'
tags:
  - 进程间通信
  - 网络
  - 内核
  - 操作系统
language: 'Chinese'
draft: false
heroImage: { src: './0.jpg', color: '#690600' }
---
## IO产生的原因
试想一下，两台机器通信，首先A会将消息丢到缓存区，然后缓冲区通过网络传输，丢到B的缓冲区，B再从自己的缓冲区读取，这是我们的基本。

---

## 几个概念
当看到IO相关概念，会看见几个词，同步异步，还有阻塞非阻塞，在起初我把同步和非阻塞连在一起理解，但并不是这样。

- 同步：就是自己一个人，没人通知我，我自食其力。
- 异步：有人帮我，代理人有结果通知我。
- 阻塞：我一直在等数据，对方不给我一丝丝提示，我就一直苦苦等待。
- 非阻塞：对方给予我提示，有数据我就读取，没数据对方便告诉我甭等了，咱们撤退。

咱们专业一点来讲述，或者我们换一个角度记忆会更清晰一些，**`同步、异步`是针对`用户态`和`内核态`的交互状态，而`阻塞、非阻塞`是针对`函数的行为`。**

- 同步 / 异步：是从用户态与内核态交互的角度来说的（是否自己等待数据拷贝完成）。
  - 同步：应用必须自己等（阻塞 I/O、非阻塞 I/O、I/O 复用、信号驱动 I/O 都是同步 I/O）。
  - 异步：应用不需要自己等，内核完成数据拷贝后通知应用（Linux AIO / io_uring）。
- 阻塞 / 非阻塞：是从函数调用行为来说的（调用是立即返回，还是要等）。

---

## 阻塞IO
流程：
1. 用户态向内核态发起读取数据
2. 准备数据的时候用户态进程阻塞
3. 准备完成复制到用户态
4. 复制完成后，返回成功提示

## 非阻塞IO
流程：
1. 用户态向内核态发起读取数据
2. 有数据包就绪就复制到用户态且返回成功码，否则还是返回错误码，过段时间再来询问

---

## IO复用模型
以上情况，对于单线程当然是极好的，但是在并发场景下，每次请求都对应一个线程，倘若有海量的信息发送或者接收，系统调用的次数和线程资源的浪费会造成大量操作冗余。于是就引出了复用技术 —— 用户线程阻塞在一个系统调用上，令某个内核线程监听多个 FD 的事件。

这里引用一篇[文章](https://mp.weixin.qq.com/s?__biz=MzkxMDc1MDg1Nw==&mid=2247508528&idx=1&sn=ca2920020af8b51c3649d103dd7d3331&source=41&poc_token=HBhssGijGVp4vreCqjw_TzS3xJ-6QO1o2Uta0Ncs)来修正我的片面理解，他在尾声说：

> 比如好多文章说，多路复用之所以效率高，是因为用一个线程就可以监控多个文件描述符。  
> 这显然是知其然而不知其所以然，多路复用产生的效果，完全可以由用户态去遍历文件描述符并调用其非阻塞的 read 函数实现。  
> 而多路复用快的原因在于，操作系统提供了这样的系统调用，使得原来的 while 循环里多次系统调用，变成了一次系统调用 + 内核层遍历这些文件描述符。  
> 就好比我们平时写业务代码，把原来 while 循环里调 http 接口进行批量，改成了让对方提供一个批量添加的 http 接口，然后我们一次 rpc 请求就完成了批量添加。  
> 一个道理。

那现在明白了，多路复用快的真正原因是**减少了系统调用次数 + 内核态优化遍历**，而不是单纯因为一个线程监控多个 fd。

---

## 信号驱动IO模型
IO复用模型采用线程主动轮训监听，而信号驱动则是与线程建立信号联系，数据准备好的时候，发送可读信号，然后才去读。

- 内核帮你维护了“数据就绪状态”，用户线程不需要自己轮询。读取仍然需要一次系统调用，但避免了无意义的轮询开销。
- 适合数据到达时间不确定的场景，减少CPU空转。

---

## 异步IO(AIO)
可以发现以上两种模型，会去发起两段请求，一问准备好了没，二把数据拿过来。所以便出现了这种模型，为了一劳永逸，用户态发起请求，内核会建立信号连接，当数据就绪，内核态会主动推送过来，这样就通过一次请求把`数据拷贝(状态询问+读取)`都完成了。

- 异步 IO 让用户态一次请求完成所有操作。
- 内核直接处理数据准备和拷贝，用户态只负责接收结果。
- 相比信号驱动和IO复用，异步IO进一步减少系统调用次数和线程阻塞，提高并发处理效率，尤其适合高并发、大数据量场景。

Linux5.7之后使用这种IO，它通过`io_uring`实现，具体如下：

1. **共享环形队列**：
    - **Submission Queue (SQ)**：用户态写入 IO 请求，内核读取执行。
    - **Completion Queue (CQ)**：内核写入完成事件，用户态轮询或等待获取结果。
    - SQ/CQ 通过 `mmap` 映射到用户态，无需每次系统调用。
2. **提交流程**：
   ```text
   用户态写入 SQ -> 内核读取 -> 调用驱动/文件系统执行 IO -> 完成写入 CQ
    ```


* 可批量提交，减少上下文切换。

3. **完成流程**：

    * 内核将事件写入 CQ。
    * 用户态协程可挂起等待，内核完成后通过 CQ 唤醒对应协程。


### 碎碎念
很遗憾写得有点糟糕啊，我没有深刻考究源码的习惯，该文并没有代码走读，所以只能用文字阐述说明。
这里当作我写到这里和你看到这里的彩蛋，您辛苦了，我也辛苦了，我们都很棒。

<details>
<summary>顺带一提，这里算是补充吧 🎉</summary>

io_uring的目标并不仅仅是高性能，初衷就如同小标题一致是想建立Linux原生的 **`异步`** IO模型。
它的根源在于它的数据结构，通过 **mmap**的`ring buffer`实现，从结果上而言：

* 减少了数据拷贝
* 减少了系统调用，一次调用可以提交或首个多次结果，上下文切换开销很小
* 通过两个环：提交队列（SQ）和完成队列（CQ），完美实现生产者-消费者模型。其底层通过内存屏障（Memory Barrier）和顺序写（Sequential Write）来保证一致性：生产者（应用）向SQ尾写入条目后，使用写屏障确保数据可见性，然后更新尾指针；消费者（内核）从SQ头读取，使用读屏障确保读取到最新数据，之后更新头指针。完成环（CQ）同理，但角色互换。这种设计确保了无锁并发，彻底避免了传统锁机制带来的CPU暂停、上下文切换和缓存失效等开销。

对比epoll，epoll基于树结构实现同步就绪通知，属于回调范式。io_uring真正实现了无阻塞异步，尤其适合与协程结合。

谈谈一种高效的结合，可以将io_uring与coroutine结合，当协程执行IO时，自动让出CPU，由用户态调度器切换至同线程其他协程。IO由内核异步执行，完成后唤醒原协程。整个过程几乎没有线程阻塞，CPU时间几乎完全用于业务，易跑满CPU。

协程的真正优势在于单线程调度，如Python。io_uring加持下，每线程可类似独立事件循环，实现高并发IO，更轻量、高效，避免多线程锁竞争。

撤退撤退，风紧撤呼 (＾• ω •＾)
</details>
```
